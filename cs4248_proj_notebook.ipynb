{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965d6de2-4541-4a8d-9b7e-645bbb55a9ca",
   "metadata": {},
   "source": [
    "# CS4248 Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfd11f-736d-4bb6-8a20-7823d75212bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch import nn, tensor, zeros, argmax\n",
    "import torch.nn.functional as F\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c00cc-75f4-4577-a68e-c74c763608aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['category', 'text']\n",
    "\n",
    "train = pd.read_csv('fulltrain.csv', names=column_names)\n",
    "X_train = train['text']\n",
    "y_train = train['category']\n",
    "\n",
    "# use 1/10 of the data for testing\n",
    "X_train = X_train[:int(len(X_train)/10)]\n",
    "y_train = y_train[:int(len(y_train)/10)]\n",
    "\n",
    "test = pd.read_csv('balancedtest.csv', names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf36a2-aeaa-411d-a074-753945d66551",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### Current\n",
    "1. Tokenize the text\n",
    "2. Lowercasing\n",
    "3. Remove punctuation\n",
    "4. Remove stop words\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6cb317-c136-48fb-b5c8-4eada6e6cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lemmatize=False, remove_stopwords=False, remove_punctuations=False):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    if remove_punctuations:\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # add EOT token\n",
    "    tokens.append('<EOT>')\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776aa71f",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embed_dim, hidden_dim, enc_drop, dec_drop, k, word_limit, eot_index):\n",
    "        super(Generator, self).__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.enc_drop = enc_drop\n",
    "        self.dec_drop = dec_drop\n",
    "        self.k = k\n",
    "        self.word_limit = word_limit\n",
    "        self.eot_index = eot_index\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Embedding(vocabulary_size, embed_dim),\n",
    "            nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=enc_drop)\n",
    "        )\n",
    "        \n",
    "        # Decoder setup\n",
    "        self.dec_embed = nn.Linear(vocabulary_size, embed_dim)\n",
    "        self.dec_lstm = nn.LSTM(embed_dim, hidden_dim, dropout=dec_drop, batch_first=True)\n",
    "        self.dec_softmax = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, vocabulary_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def top_k_sampling(self, probabilities, k):\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, k, dim=-1)\n",
    "        selected_indices = torch.multinomial(top_k_probs, 1)\n",
    "        selected = top_k_indices.gather(1, selected_indices)\n",
    "        del top_k_probs, top_k_indices, selected_indices\n",
    "        return selected.item()\n",
    "    \n",
    "    \n",
    "    def generate_text(self, batch_indexes, batch_size):\n",
    "        # encode the input sequence\n",
    "        _, (batch_hn, batch_cn) = self.encoder(batch_indexes)\n",
    "\n",
    "        gen_batch = []  # To store generated sequences\n",
    "        init_dist = zeros(self.vocabulary_size, device=device).unsqueeze(0)\n",
    "        init_dist[:, self.eot_index] = 1\n",
    "\n",
    "        # Generate a sequence for each item in the batch\n",
    "        for i in range(batch_size):\n",
    "            prev_dist = init_dist\n",
    "            hn, cn = batch_hn[:, i, :], batch_cn[:, i, :]  # Get initial states for this item in the batch\n",
    "            gen = []  # To store generated indices for this item\n",
    "            while True:\n",
    "                # Get the next word\n",
    "                word_tensor = self.dec_embed(prev_dist).to(device)\n",
    "                _ , (hn, cn) = self.dec_lstm(word_tensor, (hn, cn))\n",
    "                del word_tensor\n",
    "                prev_dist = self.dec_softmax(hn)\n",
    "                index = argmax(prev_dist)\n",
    "                if index == self.eot_index: break\n",
    "                gen.append(index)\n",
    "                if len(gen) == self.word_limit: break\n",
    "            gen_batch.append(gen)\n",
    "        \n",
    "        return gen_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d3613",
   "metadata": {},
   "source": [
    "## Discriminator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim, conv_channels, lstm_hidden_dim, dense_dim, dropout_prob):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        \n",
    "        # Convolutional layer expects input of shape (batch_size, channels, sequence_length),\n",
    "        # so embedding_dim is used as in_channels. Output channels set to 128.\n",
    "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=conv_channels, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # LSTM layer expects input of shape (batch_size, seq_len, features),\n",
    "        # so we need to permute the output from conv1d.\n",
    "        self.lstm = nn.LSTM(input_size=conv_channels, hidden_size=lstm_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer and dropout\n",
    "        self.dense = nn.Linear(lstm_hidden_dim, dense_dim)  # Assuming the LSTM does not return sequences\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(dense_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Conv1d expects (batch, channels, length), so permute the embedding output\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer expects (batch, seq_len, features), permute back\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Only take the output from the last LSTM cell\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        x = x.squeeze(0)  # Remove the first dimension (num_layers*num_directions)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed8517",
   "metadata": {},
   "source": [
    "## Generator-Discriminator 1 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f78898c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = X_train.apply(preprocess_text, args=(True, True,True)) # Tokenizing the text\n",
    "word_index = {word: i+1 for i, word in enumerate(set([token for tokens_list in tokens for token in tokens_list]))} # Mapping words to indices\n",
    "index_word = {i: word for word, i in word_index.items()} # Mapping indices to words\n",
    "labels = train['category']\n",
    "# get index of END OF TEXT token\n",
    "eot_index = word_index['<EOT>']\n",
    "\n",
    "indexes = [tensor([word_index.get(word, -1) for word in seq]) for seq in tokens]\n",
    "padded = nn.utils.rnn.pad_sequence(indexes, batch_first=True)  # num_seq * max_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f3c623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import BCELoss\n",
    "from torch import ones_like, zeros_like, tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "bce = BCELoss()\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "# Generator\n",
    "batch_size = 8  # Batch size\n",
    "embed_dim = 300  # Dimensionality of word embeddings\n",
    "hidden_dim = 128  # Number of features in the hidden state of the LSTM\n",
    "enc_drop = 0.2  # Dropout rate for the encoder LSTM\n",
    "dec_drop = 0.2  # Dropout rate for the decoder LSTM\n",
    "k = 5 # Number of top k words to sample from\n",
    "word_limit = 500  # Maximum number of words to generate TODO: for first batch only\n",
    "lr_gen_optim = 0.001  # Learning rate for generator optimizer\n",
    "\n",
    "# Discriminator1\n",
    "conv_channels = 512  # Number of output channels in the convolutional layer\n",
    "lstm_hidden_dim = 128  # Number of features in the hidden state of the LSTM\n",
    "dense_dim = 64  # Number of features in the dense layer\n",
    "dropout_prob = 0.2  # Dropout rate\n",
    "lr_disc1_optim = 0.001  # Learning rate for discriminator1 optimizer\n",
    "\n",
    "epochs = 10  # Number of epochs\n",
    "\n",
    "\n",
    "generator = Generator(len(word_index), embed_dim, hidden_dim, enc_drop, dec_drop, k, word_limit, eot_index).to(device)\n",
    "discriminator1 = Discriminator1(len(word_index), embed_dim, conv_channels, lstm_hidden_dim, dense_dim, dropout_prob).to(device)\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=lr_gen_optim)\n",
    "discriminator1_optimizer = optim.Adam(discriminator1.parameters(), lr=lr_disc1_optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "865ba4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                                                                                     | 0/611 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5764, -0.1385, -0.0505, -0.0155,  0.2246,  0.2152, -0.3046,  0.0536,\n",
      "         -0.1644, -0.2844,  0.0470,  0.1560,  0.1819,  0.0181,  0.4090,  0.0637,\n",
      "          0.2315, -0.5479, -0.1149,  0.4942, -0.2181,  0.5328,  0.2659,  0.3320,\n",
      "          0.4187,  0.5491, -0.0660, -0.1937, -0.7381,  0.2288, -0.2159,  0.3983,\n",
      "         -0.3553, -0.3887, -0.0486,  0.7916,  0.2442,  0.3451, -0.4720,  0.6922,\n",
      "          0.0661, -0.3164,  0.0819, -0.1844, -0.2744, -0.4213,  0.2003, -0.3274,\n",
      "         -0.1534, -0.3341, -0.0683, -0.2594, -0.6611, -0.0128, -0.2815,  0.3534,\n",
      "         -0.1112, -0.4023,  0.3342, -0.3867, -0.3317,  0.3740,  0.0852, -0.0815,\n",
      "          0.2013, -0.1442,  0.1886,  0.2806,  0.0681,  0.1278, -0.0175, -0.1051,\n",
      "          0.1093,  0.2562, -0.4183,  0.0080, -0.0065, -0.3729, -0.0696,  0.1388,\n",
      "         -0.0182,  0.5816,  0.1019,  0.4950, -0.0638,  0.2292,  0.2112,  0.0546,\n",
      "         -0.1815, -0.2357,  0.1853, -0.1283,  0.1406, -0.2116,  0.7794, -0.1390,\n",
      "         -0.5057, -0.3313,  0.2263,  0.2296, -0.2071,  0.2182,  0.0010, -0.5813,\n",
      "         -0.5790,  0.0252,  0.1425,  0.0688, -0.1226, -0.3293,  0.3018,  0.3793,\n",
      "          0.0275,  0.2737,  0.0761, -0.1525, -0.2139,  0.0860, -0.6783, -0.0933,\n",
      "         -0.0531, -0.5434,  0.2371, -0.5537,  0.1287,  0.3577,  0.5723,  0.0694]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.5764, -0.1385, -0.0505, -0.0155,  0.2246,  0.2152, -0.3046,  0.0536,\n",
      "         -0.1644, -0.2844,  0.0470,  0.1560,  0.1819,  0.0181,  0.4090,  0.0637,\n",
      "          0.2315, -0.5479, -0.1149,  0.4942, -0.2181,  0.5328,  0.2659,  0.3320,\n",
      "          0.4187,  0.5491, -0.0660, -0.1937, -0.7381,  0.2288, -0.2159,  0.3983,\n",
      "         -0.3553, -0.3887, -0.0486,  0.7916,  0.2442,  0.3451, -0.4720,  0.6922,\n",
      "          0.0661, -0.3164,  0.0819, -0.1844, -0.2744, -0.4213,  0.2003, -0.3274,\n",
      "         -0.1534, -0.3341, -0.0683, -0.2594, -0.6611, -0.0128, -0.2815,  0.3534,\n",
      "         -0.1112, -0.4023,  0.3342, -0.3867, -0.3317,  0.3740,  0.0852, -0.0815,\n",
      "          0.2013, -0.1442,  0.1886,  0.2806,  0.0681,  0.1278, -0.0175, -0.1051,\n",
      "          0.1093,  0.2562, -0.4183,  0.0080, -0.0065, -0.3729, -0.0696,  0.1388,\n",
      "         -0.0182,  0.5816,  0.1019,  0.4950, -0.0638,  0.2292,  0.2112,  0.0546,\n",
      "         -0.1815, -0.2357,  0.1853, -0.1283,  0.1406, -0.2116,  0.7794, -0.1390,\n",
      "         -0.5057, -0.3313,  0.2263,  0.2296, -0.2071,  0.2182,  0.0010, -0.5813,\n",
      "         -0.5790,  0.0252,  0.1425,  0.0688, -0.1226, -0.3293,  0.3018,  0.3793,\n",
      "          0.0275,  0.2737,  0.0761, -0.1525, -0.2139,  0.0860, -0.6783, -0.0933,\n",
      "         -0.0531, -0.5434,  0.2371, -0.5537,  0.1287,  0.3577,  0.5723,  0.0694]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|▎                                                                                                                                                            | 1/611 [00:01<19:41,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5764, -0.1385, -0.0505, -0.0155,  0.2246,  0.2152, -0.3046,  0.0536,\n",
      "         -0.1644, -0.2844,  0.0470,  0.1560,  0.1819,  0.0181,  0.4090,  0.0637,\n",
      "          0.2315, -0.5479, -0.1149,  0.4942, -0.2181,  0.5328,  0.2659,  0.3320,\n",
      "          0.4187,  0.5491, -0.0660, -0.1937, -0.7381,  0.2288, -0.2159,  0.3983,\n",
      "         -0.3553, -0.3887, -0.0486,  0.7916,  0.2442,  0.3451, -0.4720,  0.6922,\n",
      "          0.0661, -0.3164,  0.0819, -0.1844, -0.2744, -0.4213,  0.2003, -0.3274,\n",
      "         -0.1534, -0.3341, -0.0683, -0.2594, -0.6611, -0.0128, -0.2815,  0.3534,\n",
      "         -0.1112, -0.4023,  0.3342, -0.3867, -0.3317,  0.3740,  0.0852, -0.0815,\n",
      "          0.2013, -0.1442,  0.1886,  0.2806,  0.0681,  0.1278, -0.0175, -0.1051,\n",
      "          0.1093,  0.2562, -0.4183,  0.0080, -0.0065, -0.3729, -0.0696,  0.1388,\n",
      "         -0.0182,  0.5816,  0.1019,  0.4950, -0.0638,  0.2292,  0.2112,  0.0546,\n",
      "         -0.1815, -0.2357,  0.1853, -0.1283,  0.1406, -0.2116,  0.7794, -0.1390,\n",
      "         -0.5057, -0.3313,  0.2263,  0.2296, -0.2071,  0.2182,  0.0010, -0.5813,\n",
      "         -0.5790,  0.0252,  0.1425,  0.0688, -0.1226, -0.3293,  0.3018,  0.3793,\n",
      "          0.0275,  0.2737,  0.0761, -0.1525, -0.2139,  0.0860, -0.6783, -0.0933,\n",
      "         -0.0531, -0.5434,  0.2371, -0.5537,  0.1287,  0.3577,  0.5723,  0.0694]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.5764, -0.1385, -0.0505, -0.0155,  0.2246,  0.2152, -0.3046,  0.0536,\n",
      "         -0.1644, -0.2844,  0.0470,  0.1560,  0.1819,  0.0181,  0.4090,  0.0637,\n",
      "          0.2315, -0.5479, -0.1149,  0.4942, -0.2181,  0.5328,  0.2659,  0.3320,\n",
      "          0.4187,  0.5491, -0.0660, -0.1937, -0.7381,  0.2288, -0.2159,  0.3983,\n",
      "         -0.3553, -0.3887, -0.0486,  0.7916,  0.2442,  0.3451, -0.4720,  0.6922,\n",
      "          0.0661, -0.3164,  0.0819, -0.1844, -0.2744, -0.4213,  0.2003, -0.3274,\n",
      "         -0.1534, -0.3341, -0.0683, -0.2594, -0.6611, -0.0128, -0.2815,  0.3534,\n",
      "         -0.1112, -0.4023,  0.3342, -0.3867, -0.3317,  0.3740,  0.0852, -0.0815,\n",
      "          0.2013, -0.1442,  0.1886,  0.2806,  0.0681,  0.1278, -0.0175, -0.1051,\n",
      "          0.1093,  0.2562, -0.4183,  0.0080, -0.0065, -0.3729, -0.0696,  0.1388,\n",
      "         -0.0182,  0.5816,  0.1019,  0.4950, -0.0638,  0.2292,  0.2112,  0.0546,\n",
      "         -0.1815, -0.2357,  0.1853, -0.1283,  0.1406, -0.2116,  0.7794, -0.1390,\n",
      "         -0.5057, -0.3313,  0.2263,  0.2296, -0.2071,  0.2182,  0.0010, -0.5813,\n",
      "         -0.5790,  0.0252,  0.1425,  0.0688, -0.1226, -0.3293,  0.3018,  0.3793,\n",
      "          0.0275,  0.2737,  0.0761, -0.1525, -0.2139,  0.0860, -0.6783, -0.0933,\n",
      "         -0.0531, -0.5434,  0.2371, -0.5537,  0.1287,  0.3577,  0.5723,  0.0694]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|▌                                                                                                                                                            | 2/611 [00:03<17:10,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5764, -0.1385, -0.0505, -0.0155,  0.2246,  0.2152, -0.3046,  0.0536,\n",
      "         -0.1644, -0.2844,  0.0470,  0.1560,  0.1819,  0.0181,  0.4090,  0.0637,\n",
      "          0.2315, -0.5479, -0.1149,  0.4942, -0.2181,  0.5328,  0.2659,  0.3320,\n",
      "          0.4187,  0.5491, -0.0660, -0.1937, -0.7381,  0.2288, -0.2159,  0.3983,\n",
      "         -0.3553, -0.3887, -0.0486,  0.7916,  0.2442,  0.3451, -0.4720,  0.6922,\n",
      "          0.0661, -0.3164,  0.0819, -0.1844, -0.2744, -0.4213,  0.2003, -0.3274,\n",
      "         -0.1534, -0.3341, -0.0683, -0.2594, -0.6611, -0.0128, -0.2815,  0.3534,\n",
      "         -0.1112, -0.4023,  0.3342, -0.3867, -0.3317,  0.3740,  0.0852, -0.0815,\n",
      "          0.2013, -0.1442,  0.1886,  0.2806,  0.0681,  0.1278, -0.0175, -0.1051,\n",
      "          0.1093,  0.2562, -0.4183,  0.0080, -0.0065, -0.3729, -0.0696,  0.1388,\n",
      "         -0.0182,  0.5816,  0.1019,  0.4950, -0.0638,  0.2292,  0.2112,  0.0546,\n",
      "         -0.1815, -0.2357,  0.1853, -0.1283,  0.1406, -0.2116,  0.7794, -0.1390,\n",
      "         -0.5057, -0.3313,  0.2263,  0.2296, -0.2071,  0.2182,  0.0010, -0.5813,\n",
      "         -0.5790,  0.0252,  0.1425,  0.0688, -0.1226, -0.3293,  0.3018,  0.3793,\n",
      "          0.0275,  0.2737,  0.0761, -0.1525, -0.2139,  0.0860, -0.6783, -0.0933,\n",
      "         -0.0531, -0.5434,  0.2371, -0.5537,  0.1287,  0.3577,  0.5723,  0.0694]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.5764, -0.1385, -0.0505, -0.0155,  0.2246,  0.2152, -0.3046,  0.0536,\n",
      "         -0.1644, -0.2844,  0.0470,  0.1560,  0.1819,  0.0181,  0.4090,  0.0637,\n",
      "          0.2315, -0.5479, -0.1149,  0.4942, -0.2181,  0.5328,  0.2659,  0.3320,\n",
      "          0.4187,  0.5491, -0.0660, -0.1937, -0.7381,  0.2288, -0.2159,  0.3983,\n",
      "         -0.3553, -0.3887, -0.0486,  0.7916,  0.2442,  0.3451, -0.4720,  0.6922,\n",
      "          0.0661, -0.3164,  0.0819, -0.1844, -0.2744, -0.4213,  0.2003, -0.3274,\n",
      "         -0.1534, -0.3341, -0.0683, -0.2594, -0.6611, -0.0128, -0.2815,  0.3534,\n",
      "         -0.1112, -0.4023,  0.3342, -0.3867, -0.3317,  0.3740,  0.0852, -0.0815,\n",
      "          0.2013, -0.1442,  0.1886,  0.2806,  0.0681,  0.1278, -0.0175, -0.1051,\n",
      "          0.1093,  0.2562, -0.4183,  0.0080, -0.0065, -0.3729, -0.0696,  0.1388,\n",
      "         -0.0182,  0.5816,  0.1019,  0.4950, -0.0638,  0.2292,  0.2112,  0.0546,\n",
      "         -0.1815, -0.2357,  0.1853, -0.1283,  0.1406, -0.2116,  0.7794, -0.1390,\n",
      "         -0.5057, -0.3313,  0.2263,  0.2296, -0.2071,  0.2182,  0.0010, -0.5813,\n",
      "         -0.5790,  0.0252,  0.1425,  0.0688, -0.1226, -0.3293,  0.3018,  0.3793,\n",
      "          0.0275,  0.2737,  0.0761, -0.1525, -0.2139,  0.0860, -0.6783, -0.0933,\n",
      "         -0.0531, -0.5434,  0.2371, -0.5537,  0.1287,  0.3577,  0.5723,  0.0694]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m predicted_fake_d1 \u001b[38;5;241m=\u001b[39m discriminator1\u001b[38;5;241m.\u001b[39mforward(gen_batch_padded)\n\u001b[1;32m     12\u001b[0m generator_loss \u001b[38;5;241m=\u001b[39m bce(predicted_fake_d1, ones_like(predicted_fake_d1))\n\u001b[0;32m---> 13\u001b[0m \u001b[43mgenerator_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m generator_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the discriminator1\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i in tqdm(range(0, len(padded), batch_size), desc=\"Training\", leave=False):\n",
    "        batch_size = min(batch_size, len(padded) - i)\n",
    "        batch_indexes = padded[i:i+batch_size].to(device)  # Get a batch of sequences batch_size * max_seq\n",
    "\n",
    "        # Train the generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        gen_batch = generator.generate_text(batch_indexes, batch_size)\n",
    "        gen_batch_padded = nn.utils.rnn.pad_sequence([tensor(seq) for seq in gen_batch], batch_first=True).to(device)\n",
    "        predicted_fake_d1 = discriminator1.forward(gen_batch_padded)\n",
    "        generator_loss = bce(predicted_fake_d1, ones_like(predicted_fake_d1))\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "    \n",
    "\n",
    "        # Train the discriminator1\n",
    "        discriminator1_optimizer.zero_grad()\n",
    "        predicted_human_d1 = discriminator1.forward(batch_indexes)\n",
    "        predictions = torch.cat((predicted_fake_d1.detach(), predicted_human_d1), dim=0)\n",
    "        labels = torch.cat((zeros_like(predicted_fake_d1), ones_like(predicted_human_d1)), dim=0)\n",
    "        discriminator1_loss = bce(predictions, labels)\n",
    "        discriminator1_loss.backward()\n",
    "        discriminator1_optimizer.step()\n",
    "\n",
    "        # Cleanup to free memory\n",
    "        del batch_indexes, gen_batch, gen_batch_padded, predicted_fake_d1, predicted_human_d1, predictions, labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
