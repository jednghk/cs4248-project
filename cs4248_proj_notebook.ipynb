{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965d6de2-4541-4a8d-9b7e-645bbb55a9ca",
   "metadata": {},
   "source": [
    "# CS4248 Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfd11f-736d-4bb6-8a20-7823d75212bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch import nn, tensor, zeros, argmax\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c00cc-75f4-4577-a68e-c74c763608aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['category', 'text']\n",
    "\n",
    "train = pd.read_csv('fulltrain.csv', names=column_names)\n",
    "X_train = train['text']\n",
    "y_train = train['category']\n",
    "y_train = y_train.astype('int16')\n",
    "\n",
    "#! use 1/10 of the training data\n",
    "X_train = X_train[:len(X_train)//10]\n",
    "y_train = y_train[:len(y_train)//10]\n",
    "\n",
    "test = pd.read_csv('balancedtest.csv', names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49889d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "vocab_size = 8_000\n",
    "# Generator\n",
    "batch_size = 64  # Batch size\n",
    "embed_dim = 300  # Dimensionality of word embeddings\n",
    "hidden_dim = 128  # Number of features in the hidden state of the LSTM\n",
    "enc_drop = 0.2  # Dropout rate for the encoder LSTM\n",
    "dec_drop = 0.2  # Dropout rate for the decoder LSTM\n",
    "k = 5 # Number of top k words to sample from\n",
    "word_limit = 512  # Maximum number of words to generate TODO: for first batch only\n",
    "lr_gen_optim = 0.01  # Learning rate for generator optimizer\n",
    "\n",
    "# Discriminator1\n",
    "conv_channels = 128  # Number of output channels in the convolutional layer\n",
    "lstm_hidden_dim = 128  # Number of features in the hidden state of the LSTM\n",
    "dense_dim = 64  # Number of features in the dense layer\n",
    "dropout_prob = 0.2  # Dropout rate\n",
    "lr_disc1_optim = 0.001  # Learning rate for discriminator1 optimizer\n",
    "\n",
    "epochs = 10  # Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf36a2-aeaa-411d-a074-753945d66551",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### Current\n",
    "We make use of `SentencePieceTrainer` to train a SentencePiece model on all of the training data. We then use this model to tokenize the data. We set a vocabulary size of 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a76929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to extract the text from the CSV file\n",
    "\n",
    "# # Path to your CSV file\n",
    "# csv_file_path = 'fulltrain.csv'\n",
    "# # Path to the output text file\n",
    "# text_file_path = 'fulltrain_textonly.txt'\n",
    "\n",
    "# # Load the CSV file\n",
    "# df = pd.read_csv(csv_file_path, names=column_names)\n",
    "\n",
    "# # Assuming the text column is named 'text'. Adjust if your column name is different\n",
    "# texts = df['text']\n",
    "\n",
    "# # Save the text column to a plain text file\n",
    "# with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#     for text in texts:\n",
    "#         f.write(text + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create the SentencePiece model and save it to a file\n",
    "# We only need to run this once to create the model file\n",
    "\n",
    "# spm.SentencePieceTrainer.train(input=\"fulltrain_textonly.txt\", \n",
    "#                                model_prefix='spm_model', \n",
    "#                                vocab_size=vocab_size, \n",
    "#                                max_sentence_length=100_000,\n",
    "#                                user_defined_symbols=['<sot>'],\n",
    "#                                unk_id=0, bos_id=-1, eos_id=1, pad_id=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776aa71f",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embed_dim, hidden_dim, enc_drop, dec_drop, k, word_limit, eot_index, sot_index):\n",
    "        super(Generator, self).__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.enc_drop = enc_drop\n",
    "        self.dec_drop = dec_drop\n",
    "        self.k = k\n",
    "        self.word_limit = word_limit\n",
    "        self.eot_index = eot_index\n",
    "        self.sot_index = sot_index\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc_embed = nn.Embedding(vocabulary_size, embed_dim)\n",
    "        self.enc_lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=enc_drop)\n",
    "        \n",
    "        # Decoder setup\n",
    "        self.dec_embed = nn.Linear(vocabulary_size, embed_dim)\n",
    "        self.dec_lstm = nn.LSTM(embed_dim, hidden_dim, dropout=dec_drop, batch_first=True)\n",
    "        self.dec_softmax = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, vocabulary_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def top_k_sampling(self, probabilities, k):\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, k, dim=-1)\n",
    "        selected_indices = torch.multinomial(top_k_probs, 1)\n",
    "        selected = top_k_indices.gather(1, selected_indices)\n",
    "        return selected.item()\n",
    "    \n",
    "    \n",
    "    def generate_text(self, batch_indexes, batch_sequence_lengths, batch_size):\n",
    "        batch_sequence_lengths, perm_idx = torch.sort(torch.tensor(batch_sequence_lengths), descending=True)\n",
    "        batch_indexes = batch_indexes[perm_idx]\n",
    "        batch_embeddings = self.enc_embed(batch_indexes)\n",
    "        batch_sequence_lengths = batch_sequence_lengths.to(device)\n",
    "        packed_input = pack_padded_sequence(batch_embeddings, batch_sequence_lengths.cpu(), batch_first=True)\n",
    "\n",
    "        # encode the input sequence\n",
    "        _, (batch_hn, batch_cn) = self.enc_lstm(packed_input)\n",
    "\n",
    "        gen_batch = []  # To store generated sequences\n",
    "        init_dist = zeros(self.vocabulary_size, device=device).unsqueeze(0)\n",
    "        init_dist[:, self.sot_index] = 1\n",
    "\n",
    "        # Generate a sequence for each item in the batch\n",
    "        for i in range(batch_size):\n",
    "            prev_dist = init_dist\n",
    "            hn, cn = batch_hn[:, i, :], batch_cn[:, i, :]  # Get initial states for this item in the batch\n",
    "            gen = [tensor(self.sot_index)]  # To store generated indices for this item\n",
    "            while True:\n",
    "                # Get the next word\n",
    "                word_tensor = self.dec_embed(prev_dist).to(device)\n",
    "                _ , (hn, cn) = self.dec_lstm(word_tensor, (hn, cn))\n",
    "                del word_tensor\n",
    "                prev_dist = self.dec_softmax(hn)\n",
    "                index = argmax(prev_dist)\n",
    "                gen.append(index)\n",
    "                if index == self.eot_index: break\n",
    "                if len(gen) == self.word_limit: break\n",
    "            gen_batch.append(gen)\n",
    "        \n",
    "        return gen_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d3613",
   "metadata": {},
   "source": [
    "## Discriminator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim, conv_channels, lstm_hidden_dim, dense_dim, dropout_prob):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        \n",
    "        # Convolutional layer expects input of shape (batch_size, channels, sequence_length),\n",
    "        # so embedding_dim is used as in_channels. Output channels set to 128.\n",
    "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=conv_channels, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # LSTM layer expects input of shape (batch_size, seq_len, features),\n",
    "        # so we need to permute the output from conv1d.\n",
    "        self.lstm = nn.LSTM(input_size=conv_channels, hidden_size=lstm_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer and dropout\n",
    "        self.dense = nn.Linear(lstm_hidden_dim, dense_dim)  # Assuming the LSTM does not return sequences\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(dense_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Conv1d expects (batch, channels, length), so permute the embedding output\n",
    "        x = embedded.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer expects (batch, seq_len, features), permute back\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Pass the packed sequences through the LSTM.\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        # Pass through the fully connected layer and output layer\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed8517",
   "metadata": {},
   "source": [
    "## Generator-Discriminator 1 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78898c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
    "\n",
    "eot_index = sp.eos_id()\n",
    "sot_index = sp.piece_to_id('<sot>')\n",
    "pad_index = sp.pad_id()\n",
    "tokens = [[sot_index] + sp.encode(text, out_type=int) + [eot_index] for text in X_train]\n",
    "tokens = [torch.tensor(token, dtype=int) for token in tokens]\n",
    "sequence_lengths = [len(token) for token in tokens]\n",
    "padded = nn.utils.rnn.pad_sequence(tokens, batch_first=True, padding_value=pad_index)  # num_seq * max_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import BCELoss\n",
    "from torch import ones_like, zeros_like, tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "bce = BCELoss()\n",
    "\n",
    "generator = Generator(vocab_size, embed_dim, hidden_dim, enc_drop, dec_drop, k, word_limit, eot_index, sot_index).to(device)\n",
    "discriminator1 = Discriminator1(vocab_size, embed_dim, conv_channels, lstm_hidden_dim, dense_dim, dropout_prob).to(device)\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=lr_gen_optim)\n",
    "discriminator1_optimizer = optim.Adam(discriminator1.parameters(), lr=lr_disc1_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_losses = []\n",
    "discriminator1_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i in tqdm(range(0, len(padded), batch_size), desc=\"Training\", leave=False):\n",
    "        batch_size = min(batch_size, len(padded) - i)\n",
    "        batch_indexes = padded[i:i+batch_size].to(device)  # Get a batch of sequences batch_size * max_seq\n",
    "        batch_sequence_lengths = sequence_lengths[i:i+batch_size]  # Get the sequence lengths for this batch\n",
    "\n",
    "        # Train the generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        gen_batch = generator.generate_text(batch_indexes, batch_sequence_lengths, batch_size)\n",
    "        gen_batch_padded = nn.utils.rnn.pad_sequence([tensor(seq) for seq in gen_batch], batch_first=True, padding_value=pad_index).to(device)\n",
    "        predicted_fake_d1 = discriminator1.forward(gen_batch_padded)\n",
    "        generator_loss = bce(predicted_fake_d1, ones_like(predicted_fake_d1))\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        discriminator1_losses.append(generator_loss.item())  # Store generator loss\n",
    "\n",
    "        # Train the discriminator1\n",
    "        discriminator1_optimizer.zero_grad()\n",
    "        predicted_human_d1 = discriminator1.forward(batch_indexes)\n",
    "        predictions = torch.cat((predicted_fake_d1.detach(), predicted_human_d1), dim=0)\n",
    "        labels = torch.cat((zeros_like(predicted_fake_d1), ones_like(predicted_human_d1)), dim=0)\n",
    "        discriminator1_loss = bce(predictions, labels)\n",
    "        discriminator1_loss.backward()\n",
    "        discriminator1_optimizer.step()\n",
    "\n",
    "        generator_losses.append(discriminator1_loss.item())  # Store discriminator loss\n",
    "\n",
    "        # Cleanup to free memory\n",
    "        del batch_indexes, gen_batch, predicted_fake_d1, predicted_human_d1, predictions, labels\n",
    "\n",
    "\n",
    "\n",
    "torch.save(generator.state_dict(), \"generator_test_1.pth\")\n",
    "torch.save(discriminator1.state_dict(), \"discriminator1_test_1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34d6f1",
   "metadata": {},
   "source": [
    "### Test code region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(generator_losses, label=\"Generator\")\n",
    "plt.plot(discriminator1_losses, label=\"Discriminator\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"losses.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4977be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "test = padded[83:99].to(device)\n",
    "generated_texts = generator.generate_text(test, sequence_lengths[83:99], len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated text {i+1}:\")\n",
    "    print(text[1:-1])\n",
    "    print(sp.decode([token.item() for token in text[1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def model_memory_usage_in_MB(model):\n",
    "    # Calculate the number of elements in the model parameters\n",
    "    num_params = sum(param.numel() for param in model.parameters())\n",
    "    \n",
    "    # Assuming parameters are stored as 32-bit floats (4 bytes each), calculate memory usage in bytes\n",
    "    memory_usage_bytes = num_params * 4\n",
    "    \n",
    "    # Convert bytes to megabytes\n",
    "    memory_usage_MB = memory_usage_bytes / (1024 ** 2)\n",
    "    \n",
    "    return memory_usage_MB\n",
    "\n",
    "generator_memory = model_memory_usage_in_MB(generator)\n",
    "discriminator_memory = model_memory_usage_in_MB(discriminator1)\n",
    "\n",
    "print(f\"Generator Memory Usage: {generator_memory:.2f} MB\")\n",
    "print(f\"Discriminator Memory Usage: {discriminator_memory:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
