{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965d6de2-4541-4a8d-9b7e-645bbb55a9ca",
   "metadata": {},
   "source": [
    "# CS4248 Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfd11f-736d-4bb6-8a20-7823d75212bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch import nn, tensor, zeros, argmax\n",
    "import torch.nn.functional as F\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c00cc-75f4-4577-a68e-c74c763608aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['category', 'text']\n",
    "\n",
    "train = pd.read_csv('fulltrain.csv', names=column_names)\n",
    "X_train = train['text']\n",
    "y_train = train['category']\n",
    "test = pd.read_csv('balancedtest.csv', names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf36a2-aeaa-411d-a074-753945d66551",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### Current\n",
    "1. Tokenize the text\n",
    "2. Lowercasing\n",
    "3. Remove punctuation\n",
    "4. Remove stop words\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6cb317-c136-48fb-b5c8-4eada6e6cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lemmatize=False, remove_stopwords=False, remove_punctuations=False):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    if remove_punctuations:\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # add EOT token\n",
    "    tokens.append('<EOT>')\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776aa71f",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embed_dim, hidden_dim, enc_drop, dec_drop, k, word_limit, eot_index):\n",
    "        super(Generator, self).__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.enc_drop = enc_drop\n",
    "        self.dec_drop = dec_drop\n",
    "        self.k = k\n",
    "        self.word_limit = word_limit\n",
    "        self.eot_index = eot_index\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Embedding(vocabulary_size, embed_dim),\n",
    "            nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=enc_drop)\n",
    "        )\n",
    "        \n",
    "        # Decoder setup\n",
    "        self.dec_embed = nn.Linear(vocabulary_size, embed_dim)\n",
    "        self.dec_lstm = nn.LSTM(embed_dim, hidden_dim, dropout=dec_drop)\n",
    "        self.dec_softmax = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, vocabulary_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def top_k_sampling(self, probabilities, k):\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, k, dim=-1)\n",
    "        selected_indices = torch.multinomial(top_k_probs, 1)\n",
    "        selected = top_k_indices.gather(1, selected_indices)\n",
    "        del top_k_probs, top_k_indices, selected_indices\n",
    "        return selected.item()\n",
    "    \n",
    "    \n",
    "    def generate_text(self, batch_indexes, batch_size):\n",
    "        # encode the input sequence\n",
    "        _, (batch_hn, batch_cn) = self.encoder(batch_indexes)\n",
    "\n",
    "        gen_batch = []  # To store generated sequences\n",
    "        init_dist = zeros(self.vocabulary_size, device=device).unsqueeze(0)\n",
    "        init_dist[:, self.eot_index] = 1\n",
    "\n",
    "        # Generate a sequence for each item in the batch\n",
    "        for i in range(batch_size):\n",
    "            prev_dist = init_dist\n",
    "            hn, cn = batch_hn[:, i, :], batch_cn[:, i, :]  # Get initial states for this item in the batch\n",
    "            gen = []  # To store generated indices for this item\n",
    "            while True:\n",
    "                torch.cuda.empty_cache()\n",
    "                # Get the next word\n",
    "                word_tensor = self.dec_embed(prev_dist).to(device)\n",
    "                _ , (hn, cn) = self.dec_lstm(word_tensor, (hn, cn))\n",
    "                del word_tensor\n",
    "                prev_dist = self.dec_softmax(hn)\n",
    "                index = self.top_k_sampling(prev_dist, self.k)\n",
    "                if index == self.eot_index: break\n",
    "                gen.append(index)\n",
    "                if len(gen) == self.word_limit: break\n",
    "            gen_batch.append(gen)\n",
    "        \n",
    "        return gen_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d3613",
   "metadata": {},
   "source": [
    "## Discriminator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim, conv_channels, lstm_hidden_dim, dense_dim, dropout_prob):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        \n",
    "        # Convolutional layer expects input of shape (batch_size, channels, sequence_length),\n",
    "        # so embedding_dim is used as in_channels. Output channels set to 128.\n",
    "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=conv_channels, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # LSTM layer expects input of shape (batch_size, seq_len, features),\n",
    "        # so we need to permute the output from conv1d.\n",
    "        self.lstm = nn.LSTM(input_size=conv_channels, hidden_size=lstm_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer and dropout\n",
    "        self.dense = nn.Linear(lstm_hidden_dim, dense_dim)  # Assuming the LSTM does not return sequences\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(dense_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Conv1d expects (batch, channels, length), so permute the embedding output\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer expects (batch, seq_len, features), permute back\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Only take the output from the last LSTM cell\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        x = x.squeeze(0)  # Remove the first dimension (num_layers*num_directions)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed8517",
   "metadata": {},
   "source": [
    "## Generator-Discriminator 1 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78898c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = X_train.apply(preprocess_text) # Tokenizing the text\n",
    "word_index = {word: i+1 for i, word in enumerate(set([token for tokens_list in tokens for token in tokens_list]))} # Mapping words to indices\n",
    "index_word = {i: word for word, i in word_index.items()} # Mapping indices to words\n",
    "labels = train['category']\n",
    "# get index of END OF TEXT token\n",
    "eot_index = word_index['<EOT>']\n",
    "\n",
    "indexes = [tensor([word_index.get(word, -1) for word in seq]) for seq in tokens]\n",
    "padded = nn.utils.rnn.pad_sequence(indexes, batch_first=True)  # num_seq * max_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import BCELoss\n",
    "from torch import ones_like, zeros_like, tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "bce = BCELoss()\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "# Generator\n",
    "batch_size = 8  # Batch size\n",
    "embed_dim = 512  # Dimensionality of word embeddings\n",
    "hidden_dim = 128  # Number of features in the hidden state of the LSTM\n",
    "enc_drop = 0.2  # Dropout rate for the encoder LSTM\n",
    "dec_drop = 0.2  # Dropout rate for the decoder LSTM\n",
    "k = 5 # Number of top k words to sample from\n",
    "word_limit = 500  # Maximum number of words to generate TODO: for first batch only\n",
    "lr_gen_optim = 0.001  # Learning rate for generator optimizer\n",
    "\n",
    "# Discriminator1\n",
    "conv_channels = 512  # Number of output channels in the convolutional layer\n",
    "lstm_hidden_dim = 128  # Number of features in the hidden state of the LSTM\n",
    "dense_dim = 64  # Number of features in the dense layer\n",
    "dropout_prob = 0.2  # Dropout rate\n",
    "lr_disc1_optim = 0.001  # Learning rate for discriminator1 optimizer\n",
    "\n",
    "epochs = 10  # Number of epochs\n",
    "\n",
    "\n",
    "generator = Generator(len(word_index), embed_dim, hidden_dim, enc_drop, dec_drop, k, word_limit, eot_index).to(device)\n",
    "discriminator1 = Discriminator1(len(word_index), embed_dim, conv_channels, lstm_hidden_dim, dense_dim, dropout_prob).to(device)\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=lr_gen_optim)\n",
    "discriminator1_optimizer = optim.Adam(discriminator1.parameters(), lr=lr_disc1_optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ba4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i in tqdm(range(0, len(padded), batch_size), desc=\"Training\", leave=False):\n",
    "        batch_size = min(batch_size, len(padded) - i)\n",
    "        batch_indexes = padded[i:i+batch_size].to(device)  # Get a batch of sequences batch_size * max_seq\n",
    "\n",
    "        # Train the generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        gen_batch = generator.generate_text(batch_indexes, batch_size)\n",
    "        gen_batch_padded = nn.utils.rnn.pad_sequence([tensor(seq) for seq in gen_batch], batch_first=True).to(device)\n",
    "        predicted_fake_d1 = discriminator1.forward(gen_batch_padded)\n",
    "        generator_loss = bce(predicted_fake_d1, zeros_like(predicted_fake_d1))\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "    \n",
    "\n",
    "        # Train the discriminator1\n",
    "        discriminator1_optimizer.zero_grad()\n",
    "        predicted_human_d1 = discriminator1.forward(batch_indexes)\n",
    "        predictions = torch.cat((predicted_fake_d1.detach(), predicted_human_d1), dim=0)\n",
    "        labels = torch.cat((zeros_like(predicted_fake_d1), ones_like(predicted_human_d1)), dim=0)\n",
    "        discriminator1_loss = bce(predictions, labels)\n",
    "        discriminator1_loss.backward()\n",
    "        discriminator1_optimizer.step()\n",
    "\n",
    "        # Cleanup to free memory\n",
    "        del batch_indexes, gen_batch, gen_batch_padded, predicted_fake_d1, predicted_human_d1, predictions, labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
